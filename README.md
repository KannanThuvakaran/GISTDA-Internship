# GISTDA Internship

*Internship project from GISTDA, by Kannan Thuvakaran. Date: 04/06/2024 - Present*


## Table of Contents

- **[Project Description](#project-description)**
    - **[Original Project](#original-project)**
    - **[Proposed Project](#proposed-project)**
- **[Literature Reading](#literature-reading)**
    - **[Resources](#resources)**
    - **[Screenshots](#screenshots)**

## Project Description

### Original Project:

The original project is to create a Visual Language Model that is able to take in a question and a satellite image 

Good VLMs are produceds with a large dataset of image-text pairs. However in Remote Sensing, there is not enough image text pairs to create a good data set. (Many multitudes lower than usual)

In remote sensing, image-level classification assigns semantic labels like 'urban', 'forest', 'agricultural land' to images. However, complications arise when images contain multiple land cover types. This requires advanced techniques combining feature extraction and machine learning for accurate classification. 

### Proposed Project:
Recent research has demonstrated that fine-tuning large vision language models on small-scale, high-quality datasets can yield impressive performance in visual and language understanding.

- COGVLM TAKES Too much to fine tune

## Literature Reading

### Resources:
- Satellite Image Deep learning | [GitHub](https://github.com/satellite-image-deep-learning)
- Visual Language Models in Remote Sensing | [GitHub](https://github.com/lzw-lzw/awesome-remote-sensing-vision-language-models.git)
- SatGPT | [GitHub](https://github.com/lalligagger/satgpt.git) | [Website](https://satgpt.net/) 
- Segment Anything Model (SAM) | [GitHub](https://github.com/facebookresearch/segment-anything.git) | [Website](https://segment-anything.com/)
- Segment-geospatial using SAM | [GitHub](https://github.com/opengeos/segment-geospatial.git) | [Website](https://samgeo.gishub.org/examples/text_prompts_batch/) | [YouTube](https://www.youtube.com/watch?v=cSDvuv1zRos&ab_channel=OpenGeospatialSolutions)
- Grounding DINO | [Website](https://huggingface.co/docs/transformers/en/model_doc/grounding-dino)
- Grounded Language-Image Pre-training (GLIP) | [Github](https://github.com/microsoft/GLIP) |
- GLIP + SAM | [Code](https://colab.research.google.com/drive/1kfdizAJiD5_t-M6yFBB6t2vzGrYg8SJc#scrollTo=p2xul283jTt7)
- Remote sensing image semantic segmantation via text guided visual models | [Paper](https://arxiv.org/pdf/2304.10597)

### Screenshots: